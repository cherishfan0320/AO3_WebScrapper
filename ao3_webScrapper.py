# -*- coding: utf-8 -*-
"""WebScrapper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UNN1qcKKqYSbJp9SpLHDyIW5fhIuI6py
"""

import bs4
import re
from bs4 import BeautifulSoup, SoupStrainer
import httplib2
import requests
import pandas as pd
import csv

# CSV
#initialize csv
def init_csv():
  row_lst = ['Fic Name', 'Author', 'Relationships', 'Link', 'Kudo_Rate', 'Warning', 'Hits', 'Chapters', 'Summary']
  with open(r'best_works.csv', "w+", newline='') as file:
  # opening the file with w+ mode truncates the file\
    file.truncate() # create empty csv
    writer = csv.writer(file)
    writer.writerow(row_lst)
    file.close()

def write_csv(title, author, relationships, link_str, hits, kudo, warning, chapters, summary):
  with open(r'best_works.csv', 'a', newline='') as file:
    writer = csv.writer(file)
    link_str = remove_brackets(link_str)
    if int(kudo)== 0 or int(hits) == 0:
      KR = 'n/a'
    else:
      KR = int(kudo)/int(hits) * 100
    summary = concate_summary_paragraph(summary)
    relationships = concate_summary_paragraph(relationships)
    writer.writerow([title, author, relationships, link_str, KR, warning, hits, chapters, summary])
    file.close()

# edit

def remove_brackets(text):
  text = text.replace('[', '')
  text = text.replace(']', '')
  return text

# functions

def parse_text_only(tag):
  if tag is None:
    return 0
  if type(tag) == bs4.element.Tag:
    return tag.get_text().strip()
  else:
    return tag.strip()

def lst_parse_text_only(tags):
  tag_lst = []
  if tags is None:
    return 0
  for tag in tags:
    if type(tag) == bs4.element.Tag:
      tag_lst.append(tag.get_text().strip())
    else:
      tag_lst.append(tag.strip())
  return tag_lst

def print_summary(sum_lst):
  print('Summary:')
  for para in sum_lst:
    print(para)

# 把lst变成paragraph
def concate_summary_paragraph(sum_lst):
  summary_para = ''
  for para in sum_lst:
    summary_para += para + ' | '
  return summary_para



##### main func
def Generate_work_data(results):
  for work in results.find_all('li', {'role': 'article'}):
    # get link, title, author
    work_data = work.find('h4', {'class': "heading"})
    link = work_data.find('a', href=True)
    title = parse_text_only(work_data.find('a', href=True))
    # print(title)
    author = parse_text_only(work_data.find('a', {'rel': 'author'}))

    # get warning and relationship tags
    work_tags = work.find('ul', {'class': "tags commas"})
    warining_tag = parse_text_only(work_tags.find('a', {'class': 'tag'}))
    relationships_tags = lst_parse_text_only(work_tags.find_all('li', {'class': 'relationships'}))

    # Get Work Summary
    try: # in case there is no summary
      work_summary = work.find('blockquote', {'class': "userstuff summary"})
      summary = lst_parse_text_only(work_summary.find_all('p'))

    except AttributeError:
        # do another thing
        continue

    # get hits, kudos, chapters, word counts, etc
    work_stats = work.find('dl', {'class': "stats"})
    hits = parse_text_only(work_stats.find('dd', {'class': 'hits'}))
    language = parse_text_only(work_stats.find('dd', {'class': 'language'}))
    chapters = parse_text_only(work_stats.find('dd', {'class': 'chapters'}))
    kudos = parse_text_only(work_stats.find('dd', {'class': 'kudos'}))
    bookmarks = parse_text_only(work_stats.find('dd', {'class': 'bookmarks'}))

    link_string = '[' + str('https://archiveofourown.org' + str(link['href'])) + ']' # parse link

    # 更新csv
    write_csv(title, author, relationships_tags, link_string, hits, kudos, warining_tag, chapters, summary)

# results需要经过多个页面Scrape

def loop_pages():
  for i in range (1, 100):
    general_url = 'https://archiveofourown.org/tags/Steve%20Harrington*s*Eddie%20Munson/works'
    URL = str(general_url) + "?page=" + str(i) # get page number url
    # print(URL)
    page = requests.get(URL)
    soup = BeautifulSoup(page.content, "html.parser")
    results = soup.find("ol", {'class':'work index group'}) #得到当前页面的所有work
    Generate_work_data(results) # loop所有work，得到相应的info然后写入csv
    print("working on page " + str(i) + "...")

# all the variables:

# KR的底线
kudo_rate_requirement = 9
# relationshisp的正则表达式？

init_csv() # 创建csv
loop_pages() # loop pages, write csvs

# 读取csv, drop invalid columns
df = pd.read_csv("best_works.csv", sep=",")
df = df.dropna()

# drop low kudo rates
indexNames = df[df['Kudo_Rate'] < kudo_rate_requirement].index
df.drop(indexNames , inplace=True)

# sort by KR and Hits
df.sort_values(["Hits", "Kudo_Rate"], axis=0, ascending=[False, False], inplace=True) # sorting bu Hits and KR
# df.sort_values(["Kudo_Rate"], axis=0, ascending=[False], inplace=True) # sorting only by KR
# 去掉所有的duplicates
df.drop_duplicates(inplace=True)

df.head(20)

# downloads
# from google.colab import files
# files.download('best_works.csv')
